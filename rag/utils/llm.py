import os
from openai import OpenAI

# 1. í”„ë¡œì íŠ¸ ì „ì—­ì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ ì‹ë³„ì (llama-serverëŠ” ì–´ë–¤ ë¬¸ìì—´ì´ë“  ìˆ˜ìš©í•©ë‹ˆë‹¤)
MODEL_NAME = "Qwen3-30B-A3B-Instruct-2507"

def get_llm_client():
    """
    llama-server(http://localhost:8080)ì™€ í†µì‹ í•˜ëŠ” 
    ìˆœìˆ˜ OpenAI SDK í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # llama-serverëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë³„ë„ì˜ API Keyê°€ í•„ìš” ì—†ìœ¼ë‚˜, 
    # SDK ê·œê²©ìƒ ì„ì˜ì˜ ê°’ì„ ë„£ì–´ì¤ë‹ˆë‹¤.
    client = OpenAI(
        base_url="http://localhost:8080/v1", 
        api_key="sk-no-key-required"
    )
    return client

# 2. (ì˜µì…˜) ì„œë²„ê°€ ì •ìƒì¸ì§€ ê°„ë‹¨íˆ í™•ì¸í•˜ëŠ” ìœ í‹¸ë¦¬í‹°
def check_server_status():
    import requests
    try:
        response = requests.get("http://localhost:8080/health")
        return response.status_code == 200
    except:
        return False

if __name__ == "__main__":
    # íŒŒì¼ ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    if check_server_status():
        print("âœ… llama-serverê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.")
        client = get_llm_client()
        print(f"ğŸš€ í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ (Base URL: {client.base_url})")
    else:
        print("âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨. llama-serverê°€ 8080 í¬íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")