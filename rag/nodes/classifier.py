"""
Classifier Node: í•œêµ­ì‚¬ ë¬¸ì œ ì—¬ë¶€ íŒë³„
"""
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from .state import MCQState
from utils.llm import llm_with_params

def classifier_node(state: MCQState):
    """
    í•œêµ­ì‚¬ ë¬¸ì œ ì—¬ë¶€ë¥¼ íŒë³„í•˜ëŠ” ë…¸ë“œ
    
    Args:
        state: MCQState
        
    Returns:
        dict: {"is_history": bool}
    """
    print(f"ğŸ§ [Classifier] í•œêµ­ì‚¬ ë¬¸ì œ ì—¬ë¶€ íŒë³„ ì¤‘...")
    prompt = ChatPromptTemplate.from_template(
        "<|im_start|>system\nì£¼ì–´ì§„ ë¬¸ì œê°€ 'í•œêµ­ì‚¬(Korean History)'ì™€ ê´€ë ¨ë˜ì—ˆëŠ”ì§€ íŒë‹¨í•˜ì—¬ YES ë˜ëŠ” NOë¡œë§Œ ë‹µí•˜ì„¸ìš”.<|im_end|>\n"
        "<|im_start|>user\nì§€ë¬¸: {paragraph}\nì§ˆë¬¸: {question}\níŒë³„:<|im_end|>\n<|im_start|>assistant\n"
    )
    
    # Note: llm_with_paramsëŠ” ì™¸ë¶€ì—ì„œ ì •ì˜ë˜ì–´ì•¼ í•¨
    # ì˜ˆ: from utils.llm import llm_with_params
    chain = prompt | llm_with_params | StrOutputParser()
    res = chain.invoke({"paragraph": state['paragraph'], "question": state['question']}).strip().upper()

    return {"is_history": "YES" in res}