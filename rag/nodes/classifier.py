#########################################################################################################
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from nodes.state import MCQState
from utils.llm import get_llm

# 1. í…œí”Œë¦¿ ì •ì˜ (ì§ì ‘ì ì¸ ChatML íƒœê·¸ëŠ” ì‚­ì œ)
ROUTER_SYS_TEMPLATE = SystemMessagePromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ê³¼ëª© ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì œê°€ "í•œêµ­ì‚¬" ê³¼ëª©ì˜ ë¬¸ì œì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

[ë¶„ë¥˜ ê¸°ì¤€]
- í•œêµ­ì‚¬ ì§€ì‹ì´ ê¼­ í•„ìš”í•œ ë¬¸ì œ: 'KOREAN_HISTORY'
- ì„¸ê³„ì‚¬, ì¼ë°˜ ë…¼ë¦¬, ë¬¸í•™, ë‹¨ìˆœ ìƒì‹ ë“±: 'GENERAL'

ê²°ê³¼ëŠ” ë°˜ë“œì‹œ 'KOREAN_HISTORY' ë˜ëŠ” 'GENERAL' ì¤‘ í•œ ë‹¨ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”."""
)

ROUTER_HUMAN_TEMPLATE = HumanMessagePromptTemplate.from_template(
    """[ì§€ë¬¸]
{paragraph}

[ì§ˆë¬¸]
{question}"""
)

# 2. ì±— í…œí”Œë¦¿ ì¡°ë¦½
ROUTER_PROMPT = ChatPromptTemplate.from_messages([
    ROUTER_SYS_TEMPLATE, 
    ROUTER_HUMAN_TEMPLATE
])

def router_node(state: MCQState):
    """
    ==== í•œêµ­ì‚¬ ë¬¸ì œì¸ì§€ / ì•„ë‹Œì§€ ë¶„ê¸°í•˜ëŠ” ë…¸ë“œ =====
    """
    llm = get_llm()
    
    # LCEL ì²´ì¸ êµ¬ì„±
    # ë³„ë„ì˜ parserê°€ ì—†ë‹¤ë©´ .contentë¡œ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    chain = ROUTER_PROMPT | llm
    
    print(f"ğŸ” [Router] ê³¼ëª© ë¶„ë¥˜ ì‹œì‘ (ID: {state['id']})")
    
    # 3. ì‹¤í–‰
    response = chain.invoke({
        "paragraph": state["paragraph"], 
        "question": state["question"]
    })
    
    # ChatLlamaCppì˜ ê²°ê³¼ëŠ” AIMessage ê°ì²´ì´ë¯€ë¡œ .content ì‚¬ìš©
    result = response.content.strip().upper()
    
    is_history = "KOREAN_HISTORY" in result
    
    print(f"ğŸ“Š [Router] ë¶„ë¥˜ ê²°ê³¼: {'í•œêµ­ì‚¬' if is_history else 'ì¼ë°˜'}")
    
    return {"is_korean_history": is_history, "retrieved_context": ""}

# ë¶„ê¸° ê²°ì • í•¨ìˆ˜ (ê¸°ì¡´ê³¼ ë™ì¼)
def route_decision(state: MCQState):
    if state.get("is_korean_history", False):
        return "retrieve"
    return "general_solve"