import re
import json
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from nodes.state import MCQState
from utils.llm import get_llm

# =========================================================
# 1. í•œêµ­ì‚¬ ì „ìš© Solver Node (RAG + CoT Guard)
# =========================================================
def ko_history_solver_node(state: MCQState):
    """
    í•œêµ­ì‚¬ ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ì‚¬ë£Œ(retrieved_context)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.
    """
    llm = get_llm()
    
    system_template = SystemMessagePromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì‚¬ë£Œ í•´ì„ì— ëŠ¥ìˆ™í•œ í•œêµ­ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì œê³µëœ <ê°œë… ë³´ì¶© ìë£Œ>ì˜ ê° ë¬¸ì„œ([1], [2] ë“±)ë¥¼ ê·¼ê±°ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ì‹­ì‹œì˜¤.

** [ì¶”ë¡  ë£¨í”„ ë°©ì§€ ë° ë¶ˆëŠ¥ ë¬¸ì œ ëŒ€ì‘ ê·œì¹™] **
1. **ë‹¨ê³„ì  ë¶„ì„(<think>)**: ë‹µë³€ ì „ <think> íƒœê·¸ ë‚´ì—ì„œ ìµœëŒ€ 7ë‹¨ê³„ê¹Œì§€ë§Œ ì¶”ë¡ í•˜ì‹­ì‹œì˜¤.
2. **ë¬´í•œ ë£¨í”„ ê¸ˆì§€**: 7ë‹¨ê³„ ë‚´ì— ê²°ë¡ ì´ ë‚˜ì§€ ì•Šìœ¼ë©´ ì¦‰ì‹œ ì¶”ë¡ ì„ ë©ˆì¶”ê³  ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ë‹µì„ ì„ íƒí•˜ì‹­ì‹œì˜¤.
3. **ìµœì¢… í˜•ì‹**: ë°˜ë“œì‹œ ë§ˆì§€ë§‰ ì¤„ì— {{"ì •ë‹µ": "ë²ˆí˜¸"}} í˜•ì‹ìœ¼ë¡œ ë‹µì„ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."""
    )

    human_template = HumanMessagePromptTemplate.from_template(
        """<ì§€ë¬¸>
{paragraph}

<ê°œë… ë³´ì¶© ìë£Œ>
{retrieved_context}

<ì§ˆë¬¸>
{question}

<ì„ ì§€>
{choices}"""
    )

    prompt = ChatPromptTemplate.from_messages([system_template, human_template])
    
    # ì„ ì§€ í¬ë§·íŒ…
    choices_str = "\n".join([f"{i+1}. {c}" for i, c in enumerate(state['choices'])])

    print(f"ğŸ¤– [History Solver] ì¶”ë¡  ì‹œì‘ (ID: {state['id']})")
    
    chain = prompt | llm
    response = chain.invoke({
        "paragraph": state['paragraph'],
        "question": state['question'],
        "choices": choices_str,
        "retrieved_context": state.get('retrieved_context', "ê´€ë ¨ ì‚¬ë£Œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    })

    return {"full_response": response.content}


# =========================================================
# 2. ì¼ë°˜ ê³¼ëª© Solver Node (Pure CoT Guard)
# =========================================================
def general_solver_node(state: MCQState):
    """
    í•œêµ­ì‚¬ ì™¸ ê³¼ëª©ì„ ìœ„í•œ ë…¸ë“œì…ë‹ˆë‹¤. ì™¸ë¶€ ìë£Œ ì—†ì´ ì§€ë¬¸ì˜ ë…¼ë¦¬ êµ¬ì¡°ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.
    """
    llm = get_llm()
    
    system_template = SystemMessagePromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë…¼ë¦¬ì ì´ê³  ê°ê´€ì ì¸ ìˆ˜í—˜ìƒì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì§€ë¬¸ë§Œì„ ë¶„ì„í•˜ì—¬ ì •ë‹µì„ ê³ ë¥´ì‹­ì‹œì˜¤.

** [ì¶”ë¡  ë° ì¢…ë£Œ ê·œì¹™] **
1. **ë‹¨ê³„ì  ì‚¬ê³ (<think>)**: <think> íƒœê·¸ ë‚´ì—ì„œ ìµœëŒ€ 7ë‹¨ê³„ì˜ ë…¼ë¦¬ ì „ê°œë¥¼ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤.
2. **ìµœì„ ì±… ì„ íƒ**: ì •ë³´ê°€ ë¶€ì¡±í•˜ë”ë¼ë„ ê°€ì¥ íƒ€ë‹¹í•´ ë³´ì´ëŠ” ë²ˆí˜¸ë¥¼ ê³ ë¥´ì‹­ì‹œì˜¤.
3. **í˜•ì‹ ì¤€ìˆ˜**: ë§ˆì§€ë§‰ ì¤„ì€ ë°˜ë“œì‹œ {{"ì •ë‹µ": "ë²ˆí˜¸"}} ì…ë‹ˆë‹¤."""
    )

    human_template = HumanMessagePromptTemplate.from_template(
        """[ì§€ë¬¸]: {paragraph}\n[ì§ˆë¬¸]: {question}\n[ì„ ì§€]:\n{choices}"""
    )

    prompt = ChatPromptTemplate.from_messages([system_template, human_template])
    choices_str = "\n".join([f"{i+1}. {c}" for i, c in enumerate(state['choices'])])

    print(f"ğŸ¤– [General Solver] ì¶”ë¡  ì‹œì‘ (ID: {state['id']})")

    chain = prompt | llm
    response = chain.invoke({
        "paragraph": state['paragraph'],
        "question": state['question'],
        "choices": choices_str
    })

    return {'full_response': response.content}


# =========================================================
# 3. Recovery Node (ë¹„ìƒ ì •ë‹µ ì¶”ì¶œ ë…¸ë“œ)
# =========================================================
def recovery_node(state: MCQState):
    """
    Solverê°€ í˜•ì‹ì„ ì§€í‚¤ì§€ ëª»í–ˆì„ ë•Œ, ì¶”ë¡  ë¡œê·¸ì—ì„œ ì •ë‹µ ë²ˆí˜¸ë¥¼ ê°•ì œë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    llm = get_llm()
    
    recovery_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì±„ì ê´€ì…ë‹ˆë‹¤. ì•„ë˜ ì¶”ë¡  ë‚´ìš©ì—ì„œ ìµœì¢… ì •ë‹µ ë²ˆí˜¸ í•˜ë‚˜ë§Œ ìˆ«ìë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."),
        ("human", "ì´ì „ ì¶”ë¡  ë‚´ìš©: {full_response}\n\nê²°êµ­ ì •ë‹µì€ ëª‡ ë²ˆì…ë‹ˆê¹Œ?")
    ])
    
    print(f"ğŸš¨ [Recovery] ë¹„ìƒ ì •ë‹µ ì¶”ì¶œ ì‹œë„ (ID: {state['id']})")
    
    # [ìˆ˜ì •] invoke ì‹œ í•„ìš”í•œ ë³€ìˆ˜(full_response)ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
    chain = recovery_prompt | llm
    response = chain.invoke({"full_response": state['full_response']})
    
    # ìˆ«ì í•˜ë‚˜ë§Œ ì¶”ì¶œ
    match = re.search(r"\d", response.content)
    final_answer = match.group(0) if match else "1" 
    
    return {"final_answer": final_answer}