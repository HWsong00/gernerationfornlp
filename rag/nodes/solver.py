import re
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from nodes.state import MCQState
from utils.llm import get_llm

# =========================================================
# 1. í•œêµ­ì‚¬ ì „ìš© Solver Node (RAG + CoT Guard)
# =========================================================
def ko_history_solver_node(state: MCQState):
    """
    í•œêµ­ì‚¬ ì „ë¬¸ê°€ í˜ë¥´ì†Œë‚˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ì‚¬ë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ í•©ë‹ˆë‹¤.
    ì¶”ë¡ ì´ ê¼¬ì´ê±°ë‚˜ 7ë‹¨ê³„ë¥¼ ë„˜ì–´ê°€ë©´ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ë‹µì„ ì„ íƒí•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.
    """
    llm = get_llm()
    
    system_template = SystemMessagePromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ì‚¬ë£Œ í•´ì„ì— ëŠ¥ìˆ™í•œ í•œêµ­ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì œê³µëœ <ê°œë… ë³´ì¶© ìë£Œ>ì˜ ê° ë¬¸ì„œ([1], [2] ë“±)ë¥¼ ê·¼ê±°ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ì‹­ì‹œì˜¤.

** [ì¶”ë¡  ë£¨í”„ ë°©ì§€ ë° ë¶ˆëŠ¥ ë¬¸ì œ ëŒ€ì‘ ê·œì¹™]**
1. **ë‹¨ê³„ì  ë¶„ì„(<think>)**: ë‹µë³€ ì „ <think> íƒœê·¸ ë‚´ì—ì„œ ìµœëŒ€ 7ë‹¨ê³„ê¹Œì§€ë§Œ ì¶”ë¡ í•˜ì‹­ì‹œì˜¤.
2. **ë¬´í•œ ë£¨í”„ ê¸ˆì§€**: ë§Œì•½ 7ë‹¨ê³„ ë‚´ì— ëª…í™•í•œ ê²°ë¡ ì´ ë‚˜ì§€ ì•Šê±°ë‚˜, ì§€ë¬¸ì˜ ì˜¤ë¥˜ë¡œ ì¸í•´ ì •ë‹µì„ í™•ì •í•  ìˆ˜ ì—†ë‹¤ë©´, ì¦‰ì‹œ ì¶”ë¡ ì„ ë©ˆì¶”ì‹­ì‹œì˜¤.
3. **ì»¨í”¼ë˜ìŠ¤ ì¶”ë¡ **: ë…¼ë¦¬ì  ì¶©ëŒì´ ë°œìƒí•  ê²½ìš°, ì§€ê¸ˆê¹Œì§€ ë¶„ì„í•œ ë‚´ìš© ì¤‘ ê°€ì¥ 'í™•ë¥ ì´ ë†’ì€(Confidence)' ì„ íƒì§€ë¥¼ ì •ë‹µìœ¼ë¡œ ì„ íƒí•˜ì—¬ ì˜¤ë‹µ ì†Œê±°ë²•ì„ ì™„ì„±í•˜ì‹­ì‹œì˜¤.
4. **ìµœì¢… í˜•ì‹**: ë°˜ë“œì‹œ ë§ˆì§€ë§‰ ì¤„ì— {{"ì •ë‹µ": "ë²ˆí˜¸"}} í˜•ì‹ìœ¼ë¡œ ë‹µì„ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."""
    )

    human_template = HumanMessagePromptTemplate.from_template(
        """<ì§€ë¬¸>
{paragraph}

<ê°œë… ë³´ì¶© ìë£Œ>
{retrieved_context}

<ì§ˆë¬¸>
{question}

<ì„ ì§€>
{choices}"""
    )

    prompt = ChatPromptTemplate.from_messages([system_template, human_template])
    choices_str = "\n".join([f"{i+1}. {c}" for i, c in enumerate(state['choices'])])

    print(f"ğŸ¤– [History Solver] ì¶”ë¡  ì‹œì‘ (ID: {state['id']})")
    
    response = (prompt | llm).invoke({
        "paragraph": state['paragraph'],
        "question": state['question'],
        "choices": choices_str,
        "retrieved_context": state.get('retrieved_context', "ê´€ë ¨ ìë£Œ ì—†ìŒ")
    })

    return {"full_response": response.content}


# =========================================================
# 2. ì¼ë°˜ ê³¼ëª© Solver Node (Pure CoT Guard)
# =========================================================
def general_solver_node(state: MCQState):
    """
    í•œêµ­ì‚¬ ì™¸ ê³¼ëª©ì„ ìœ„í•œ ë…¼ë¦¬ ì¶”ë¡  ë…¸ë“œì…ë‹ˆë‹¤.
    ì™¸ë¶€ ìë£Œ ì—†ì´ ì§€ë¬¸ì˜ ë…¼ë¦¬ êµ¬ì¡°ì— ì§‘ì¤‘í•˜ë©°, ì¶”ë¡  ì œí•œì„ ë™ì¼í•˜ê²Œ ì ìš©í•©ë‹ˆë‹¤.
    """
    llm = get_llm()
    
    system_template = SystemMessagePromptTemplate.from_template(
        """ë‹¹ì‹ ì€ ë…¼ë¦¬ì ì´ê³  ê°ê´€ì ì¸ ìˆ˜í—˜ìƒì…ë‹ˆë‹¤. ì§€ë¬¸ì„ ë¶„ì„í•˜ì—¬ ì •ë‹µì„ ê³ ë¥´ì‹­ì‹œì˜¤.

** [ì¶”ë¡  ë° ì¢…ë£Œ ê·œì¹™]**
1. **ë‹¨ê³„ì  ì‚¬ê³ (<think>)**: <think> íƒœê·¸ ë‚´ì—ì„œ ìµœëŒ€ 7ë‹¨ê³„ì˜ ë…¼ë¦¬ ì „ê°œë¥¼ ìˆ˜í–‰í•˜ì‹­ì‹œì˜¤.
2. **ìµœì„ ì±… ì„ íƒ**: ì§€ë¬¸ì— ë…¼ë¦¬ì  ê²°í•¨ì´ ìˆê±°ë‚˜ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ íŒë‹¨ì´ ë¶ˆê°€ëŠ¥í•  ê²½ìš°, ê°€ì¥ ë…¼ë¦¬ì ìœ¼ë¡œ íƒ€ë‹¹í•´ ë³´ì´ëŠ” 'ìµœì„ ì˜ ì„ íƒì§€'ë¥¼ ê³ ë¥´ê³  ì¶”ë¡ ì„ ë§ˆë¬´ë¦¬í•˜ì‹­ì‹œì˜¤.
3. **í˜•ì‹ ì¤€ìˆ˜**: ë§ˆì§€ë§‰ ì¤„ì€ ë°˜ë“œì‹œ {{"ì •ë‹µ": "ë²ˆí˜¸"}} ì…ë‹ˆë‹¤."""
    )

    human_template = HumanMessagePromptTemplate.from_template(
        """[ì§€ë¬¸]: {paragraph}\n[ì§ˆë¬¸]: {question}\n[ì„ ì§€]:\n{choices}"""
    )

    prompt = ChatPromptTemplate.from_messages([system_template, human_template])
    choices_str = "\n".join([f"{i+1}. {c}" for i, c in enumerate(state['choices'])])

    print(f"ğŸ¤– [General Solver] ì¶”ë¡  ì‹œì‘ (ID: {state['id']})")

    response = (prompt | llm).invoke({
        "paragraph": state['paragraph'],
        "question": state['question'],
        "choices": choices_str
    })

    return {'full_response': response.content}


# =========================================================
# 3. Recovery Node (ë¹„ìƒ ì •ë‹µ ì¶”ì¶œ ë…¸ë“œ)
# =========================================================
def recovery_node(state: MCQState):
    """
    Solverê°€ ë£¨í”„ë¥¼ ëŒë‹¤ ëŠê¸°ê±°ë‚˜ í˜•ì‹ì„ ì§€í‚¤ì§€ ëª»í–ˆì„ ë•Œ, 
    ì´ì „ê¹Œì§€ì˜ ì¶”ë¡  ë¡œê·¸ì—ì„œ ì •ë‹µ ë²ˆí˜¸ë¥¼ ê°•ì œë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    llm = get_llm()
    
    recovery_prompt = ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì±„ì ê´€ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ë³µì¡í•œ ì¶”ë¡  ê³¼ì •ì—ì„œ ëª¨ë¸ì´ ìµœì¢…ì ìœ¼ë¡œ ë„ë‹¬í•˜ê³ ì í–ˆë˜ 'ê°€ì¥ ìœ ë ¥í•œ ì •ë‹µ ë²ˆí˜¸' í•˜ë‚˜ë§Œ ì„ íƒí•˜ì‹­ì‹œì˜¤. ë°˜ë“œì‹œ ìˆ«ìë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."),
        ("human", f"ì´ì „ ì¶”ë¡  ë‚´ìš©: {state['full_response']}\n\nê²°êµ­ ì •ë‹µì€ ëª‡ ë²ˆì…ë‹ˆê¹Œ?")
    ])
    
    print(f" [Recovery] ë¹„ìƒ ì •ë‹µ ì¶”ì¶œ ì‹œë„ (ID: {state['id']})")
    
    response = (recovery_prompt | llm).invoke({})
    
    # ìˆ«ì í•˜ë‚˜ë§Œ ì¶”ì¶œ (ì •ê·œì‹)
    match = re.search(r"\d", response.content)
    final_answer = match.group(0) if match else "1" # ìµœí›„ì˜ ë³´ë£¨: 1ë²ˆ
    
    return {"final_answer": final_answer}