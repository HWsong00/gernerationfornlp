import json
from nodes.state import MCQState
from utils.llm import get_llm_client, MODEL_NAME
from utils.wiki import WikipediaAPI, WikiChunker 

def retrieve_node(state: MCQState, ensemble_retriever, reranker):
    """
    ==== ê³ ë„í™”ëœ ì§€ì‹ ê²€ìƒ‰ ë…¸ë“œ (Native OpenAI SDK / No Truncation) ====
    """
    client = get_llm_client()
    
    # --- Phase 1: ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ (Native JSON Mode) ---
    system_content = (
        "ë‹¹ì‹ ì€ ê²€ìƒ‰ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì œ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ìœ„í‚¤í”¼ë””ì•„ ê²€ìƒ‰ì— ìµœì í™”ëœ í•µì‹¬ ìš©ì–´ 3ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.\n"
        "ë°˜ë“œì‹œ ['í‚¤ì›Œë“œ1', 'í‚¤ì›Œë“œ2', 'í‚¤ì›Œë“œ3'] í˜•ì‹ì˜ JSON ë¦¬ìŠ¤íŠ¸ë¡œë§Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤."
    )
    
    messages = [
        {"role": "system", "content": system_content},
        {"role": "user", "content": f"ì§€ë¬¸: {state.get('paragraph', '')}\nì§ˆë¬¸: {state.get('question', '')}"}
    ]

    print(f"ğŸ”‘ [Retriever] í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘ (ID: {state.get('id', 'unknown')})")
    
    try:
        kw_response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            temperature=0,
            response_format={"type": "json_object"},
            max_tokens=150  # í‚¤ì›Œë“œê°€ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì—¬ìœ  ìˆê²Œ ì„¤ì •
        )
        
        raw_content = kw_response.choices[0].message.content.strip()
        parsed_json = json.loads(raw_content)
        
        # JSON êµ¬ì¡°ì— ë”°ë¥¸ ìœ ì—°í•œ íŒŒì‹±
        if isinstance(parsed_json, list):
            search_queries = parsed_json
        elif isinstance(parsed_json, dict) and 'keywords' in parsed_json:
            search_queries = parsed_json['keywords']
        else:
            search_queries = list(parsed_json.values())[0] if parsed_json else []
            
        if not search_queries: raise ValueError("Empty keywords")
        
    except Exception as e:
        # [ìˆ˜ì •] ë„ˆë¬´ ê¸¸ë©´ ìë¥´ëŠ” ë¡œì§ ì œê±°: ì›ë³¸ ì§ˆë¬¸ ì „ì²´ë¥¼ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
        print(f"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨({e}), ì§ˆë¬¸ ì „ì²´ë¥¼ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        search_queries = [state.get('question', '')]

    print(f"ğŸ” [Retriever] ìµœì¢… ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸: {search_queries}")

    # --- Phase 2: ë‹¤ì¤‘ ì¶œì²˜ ê²€ìƒ‰ (Local + Wiki) ---
    candidate_docs = []
    
    # 1. ë¡œì»¬ ì•™ìƒë¸” ê²€ìƒ‰
    for query in search_queries[:2]:
        candidate_docs.extend(ensemble_retriever.invoke_ensemble(query))

    # 2. ìœ„í‚¤ë°±ê³¼ ê²€ìƒ‰
    try:
        wiki_api = WikipediaAPI()
        chunker = WikiChunker()
        wiki_raw = wiki_api.search_and_fetch(search_queries)
        wiki_chunks = chunker.chunk(wiki_raw)
        for ch in wiki_chunks[:10]:
            candidate_docs.append(ch['text']) 
    except Exception as e:
        print(f"âŒ ìœ„í‚¤ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")

    # ì¤‘ë³µ ì œê±° ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
    raw_texts = []
    for d in candidate_docs:
        text = d.page_content if hasattr(d, 'page_content') else d
        if text not in raw_texts:
            raw_texts.append(text)

    # --- Phase 3: ë¦¬ë­í‚¹ (Reranking) ---
    print(f"âš–ï¸ [Reranker] {len(raw_texts)}ê°œ ë¬¸ì„œ ì¬ì •ë ¬ ì‹œì‘...")
    
    combined_query = f"{state['question']} {' '.join(state['choices'])}"
    reranked_results = reranker.rerank(combined_query, raw_texts, top_k=3)
    
    final_context = []
    for i, (text, score) in enumerate(reranked_results):
        final_context.append(f"[{i+1}] (ì‹ ë¢°ë„: {score:.2f}) {text}")

    context_str = "\n\n".join(final_context)
    print(f"âœ… [Retriever] ìµœì¢… ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ì™„ë£Œ")

    return {
        "retrieved_context": f"=== [ì—„ì„ ëœ ì§€ì‹ ì»¨í…ìŠ¤íŠ¸] ===\n{context_str}"
    }