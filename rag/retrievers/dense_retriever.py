import gc
import torch
import numpy as np
from typing import List, Dict, Any
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.documents import Document

class DenseResourceManager:
    def __init__(self, model_name: str = "dragonkue/BGE-m3-ko"):
        self.model_name = model_name
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = None

    def load_model(self):
        """ì„ë² ë”© ëª¨ë¸ì„ GPUì— ë¡œë“œí•©ë‹ˆë‹¤."""
        if self.model is None:
            print(f"ğŸ›°ï¸ [Dense] ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘ ({self.model_name})...")
            self.model = HuggingFaceEmbeddings(
                model_name=self.model_name,
                model_kwargs={'device': self.device},
                encode_kwargs={'batch_size': 32} # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìƒí–¥ìœ¼ë¡œ ì†ë„ ì—…
            )
        return self.model

    def unload_model(self):
        """GPU ë©”ëª¨ë¦¬ë¥¼ ì™„ì „íˆ ë¹„ì›ë‹ˆë‹¤."""
        if self.model is not None:
            print("ğŸ§¹ [Dense] ì„ë² ë”© ëª¨ë¸ ì œê±° ë° GPU ë©”ëª¨ë¦¬ í•´ì œ...")
            del self.model
            self.model = None
            gc.collect()
            torch.cuda.empty_cache()

def build_dense_retriever(splits: List[Document], resource_manager: DenseResourceManager):
    """ì „ì²´ ë¬¸ì„œ(Corpus)ë¥¼ ì¸ë±ì‹±í•˜ì—¬ ë²¡í„° DBë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    model = resource_manager.load_model()
    print(f"ğŸ“¦ [Dense] {len(splits)}ê°œ ì²­í¬ ë²¡í„° DB ìƒì„± ì‹œì‘ (GPU)...")
    
    vectorstore = Chroma.from_documents(
        documents=splits, 
        embedding=model,
        collection_name="history_dense_db"
    )
    return vectorstore

def pre_embed_eval_dataset(evaluation_data: List[Dict], resource_manager: DenseResourceManager):
    """
    869ê°œ í…ŒìŠ¤íŠ¸ì…‹ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ GPU ë°°ì¹˜ë¡œ ì‚¬ì „ ì„ë² ë”©í•©ë‹ˆë‹¤.
    ê²°ê³¼ëŠ” {ë¬¸ì œ_ID: {íƒ€ì…: ë²¡í„°}} í˜•íƒœë¡œ ë°˜í™˜í•˜ì—¬ ë‚˜ì¤‘ì— CPU ì—°ì‚° ì—†ì´ ê²€ìƒ‰í•˜ê²Œ í•©ë‹ˆë‹¤.
    """
    model = resource_manager.load_model()
    
    # 1. ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ set ì‚¬ìš© ê³ ë ¤ ê°€ëŠ¥)
    all_texts = []
    text_mapping = [] # (problem_id, type, index) ë³´ê´€ìš©
    
    for item in evaluation_data:
        p_id = item.get('id', 'unknown')
        # ì§€ë¬¸, ì§ˆë¬¸, ì„ ì§€ë“¤ ìˆœì„œëŒ€ë¡œ ì¶”ê°€
        texts_to_embed = [
            ("paragraph", item['paragraph']),
            ("question", item['question']),
        ] + [("choice", c) for c in item['choices']]
        
        for t_type, content in texts_to_embed:
            all_texts.append(content)
            text_mapping.append((p_id, t_type))

    print(f"ğŸš€ [Dense] ì´ {len(all_texts)}ê°œ í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© ì‹œì‘...")
    
    # 2. GPU ë°°ì¹˜ ì„ë² ë”© ì‹¤í–‰
    vectors = model.embed_documents(all_texts)
    
    # 3. êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì¬ì¡°ë¦½
    pre_computed_vectors = {}
    for (p_id, t_type), vector in zip(text_mapping, vectors):
        if p_id not in pre_computed_vectors:
            pre_computed_vectors[p_id] = {}
        
        # ì„ ì§€ëŠ” ì—¬ëŸ¬ ê°œì´ë¯€ë¡œ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬
        if t_type == "choice":
            if "choices" not in pre_computed_vectors[p_id]:
                pre_computed_vectors[p_id]["choices"] = []
            pre_computed_vectors[p_id]["choices"].append(vector)
        else:
            pre_computed_vectors[p_id][t_type] = vector
            
    print("âœ… [Dense] í…ŒìŠ¤íŠ¸ì…‹ ì‚¬ì „ ì„ë² ë”© ì™„ë£Œ.")
    return pre_computed_vectors