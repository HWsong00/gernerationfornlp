import glob
import json
import os
import gc
from typing import List
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

def load_and_split_data(data_path: str, chunk_size=1000, chunk_overlap=200) -> List[Document]:
    text_splitter = RecursiveCharacterTextSplitter(
        separators=["\n\n", "\n", ". ", " "], 
        chunk_size=chunk_size, 
        chunk_overlap=chunk_overlap
    )
    
    final_splits = []
    file_list = glob.glob(data_path)
    
    print(f"ğŸ“š [Data] ë°ì´í„° ë¡œë“œ ë° ì²­í‚¹ ì‹œì‘... (ëŒ€ìƒ íŒŒì¼: {len(file_list)}ê°œ)")

    for file_path in file_list:
        with open(file_path, 'r', encoding='utf-8') as f:
            # 1. íŒŒì¼ í•˜ë‚˜ë¥¼ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¼
            data = json.load(f)
            
            for item in data:
                page_content = (
                    f"ì œëª©: {item.get('ì œëª©', '')}\n"
                    f"í•œìëª…: {item.get('í•œìëª…', '')}\n"
                    f"ì •ì˜: {item.get('[ì •ì˜]', '')}\n"
                    f"ë‚´ìš©: {item.get('[ë‚´ìš©]', '')}"
                )
                
                metadata = {
                    "title": item.get('ì œëª©', ''),
                    "source": os.path.basename(file_path)
                }
                
                # 2. ì¦‰ì‹œ ì²­í‚¹í•˜ì—¬ ê²°ê³¼ë§Œ final_splitsì— ì¶”ê°€
                chunks = text_splitter.split_text(page_content)
                title_prefix = f"ë¬¸ì„œì œëª©: {metadata['title']}\n"
                
                for i, chunk in enumerate(chunks):
                    content = title_prefix + chunk if i > 0 else chunk
                    final_splits.append(Document(page_content=content, metadata={**metadata, "chunk_id": i}))
            
    if 'data' in locals():
        del data
    
    gc.collect() 
    print(f"âœ… [Data] ì´ {len(final_splits)}ê°œì˜ ì²­í¬ ìƒì„± ì™„ë£Œ. ë©”ëª¨ë¦¬ ì •ë¦¬ í›„ ë¦¬í„´í•©ë‹ˆë‹¤.")
    
    return final_splits