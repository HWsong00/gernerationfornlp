from langchain_community.llms import LlamaCpp
import os
from dotenv import load_dotenv
from huggingface_hub import hf_hub_download
import glob
from langchain_community.document_loaders import JSONLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langfuse import Langfuse
import ast
import pandas as pd
from tqdm import tqdm

MODEL_PATH = hf_hub_download(
    repo_id="unsloth/Qwen3-30B-A3B-Instruct-2507-GGUF",
    filename="Qwen3-30B-A3B-Instruct-2507-UD-Q6_K_XL.gguf",
)

# LlamaCppë¡œ ì§ì ‘ ëª¨ë¸ ë¡œë“œ
base_llm = LlamaCpp(
        model_path=MODEL_PATH,
        n_gpu_layers=-1,      # L4 GPU ì‚¬ìš©
        n_ctx=32768,          # ê¸´ ë¬¸ë§¥(RAG) ì§€ì›
        max_tokens=2048,
        temperature=0.7,
        top_p=0.90,
        repeat_penalty=1.1,
        verbose=False,        # ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
)
    
print("=====ëª¨ë¸ ë¡œë“œ ì™„ë£Œ=====")

print("=====ë°ì´í„° ë¡œë”© ì‹œì‘=====")

DATA_PATH = "...í•œêµ­ì‚¬ RAG ìë£Œ ë°ì´í„° ê²½ë¡œ..."
DB_PATH = "...chromaDB ì €ì¥ ê²½ë¡œ..."
COLLECTION_NAME = "korean_history_2"

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embedding_model = HuggingFaceEmbeddings(
    model_name="dragonkue/BGE-m3-ko",
    model_kwargs={'device': 'cuda'}
)

if os.path.exists(DB_PATH) and os.listdir(DB_PATH):
    print(f"ê¸°ì¡´ ë²¡í„° DBë¥¼ '{DB_PATH}'ì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")
    vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=embedding_model,
        collection_name=COLLECTION_NAME
    )
    print("=====ê¸°ì¡´ DB ë¡œë“œ ì™„ë£Œ=====")
    
else:
    print(f"ê¸°ì¡´ DBê°€ ì—†ìŠµë‹ˆë‹¤. '{DATA_PATH}'ì—ì„œ ìƒˆ DB êµ¬ì¶•ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    files = glob.glob(DATA_PATH)
    documents = []

    def metadata_func(record: dict, metadata: dict) -> dict:
        metadata["volume"] = record.get("volume")
        metadata["title"] = record.get("title")
        return metadata

    for file in files:
        try:
            loader = JSONLoader(
                file_path=file,
                jq_schema='.[]',
                content_key='text',
                metadata_func=metadata_func
            )
            documents.extend(loader.load())
        except Exception as e:
            print(f"{file} ë¡œë“œ ì‹¤íŒ¨: {e}")

    if documents:
        print(f"ì´ {len(documents)}ê°œì˜ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        
        # ì²­í‚¹
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(documents)
        print(f"{len(splits)}ê°œì˜ ì²­í¬ë¡œ ë¶„í• ë¨")

        # ë²¡í„° DB ìƒì„± ë° ì €ì¥
        print(f"=====ì‹ ê·œ ë²¡í„° DBë¥¼ '{DB_PATH}'ì— ìƒì„± ë° ì €ì¥ ì¤‘=====")
        vectorstore = Chroma.from_documents(
            documents=splits,
            embedding=embedding_model,
            collection_name=COLLECTION_NAME,
            persist_directory=DB_PATH
        )
        print("=====ì‹ ê·œ DB ìƒì„± ë° ì €ì¥ ì™„ë£Œ=====")
    else:
        print("ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ì–´ DBë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        vectorstore = None

# ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})
print("=====ë¦¬íŠ¸ë¦¬ë²„ ì¤€ë¹„ ì™„ë£Œ=====")

# .env íŒŒì¼ì˜ ë‚´ìš©ì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì™€ ì„¤ì • (ê°’ì´ ì—†ìœ¼ë©´ None ë°˜í™˜)
os.environ["LANGFUSE_SECRET_KEY"] = os.getenv("LANGFUSE_SECRET_KEY")
os.environ["LANGFUSE_PUBLIC_KEY"] = os.getenv("LANGFUSE_PUBLIC_KEY")
os.environ["LANGFUSE_HOST"] = os.getenv("LANGFUSE_HOST")

try:
    langfuse = Langfuse()
    if langfuse.auth_check():
        print("=====ë­í“¨ì¦ˆ ì—°ê²° ì™„ë£Œ=====")
    else:
        print("=====ë­í“¨ì¦ˆ ì—°ê²° ì‹¤íŒ¨=====")
except Exception as e:
    print(f"ì—ëŸ¬: {e}")
    
    
langfuse_handler = None
try:
    from langfuse.langchain import CallbackHandler
    langfuse_handler = CallbackHandler()
    print("=====LangFuse í•¸ë“¤ëŸ¬ ì—°ê²° ì„±ê³µ! (ë¡œê·¸ ì ì¬ ì¤‘)=====")

except ImportError:
    print("=====LangFuse íŒ¨í‚¤ì§€ ê²½ë¡œ ì—ëŸ¬: ë¡œê·¸ ì—†ì´ ì§„í–‰=====")
except Exception as e:
    print(f"=====LangFuse ì—°ê²° ì‹¤íŒ¨ ({e}): ë¡œê·¸ ì—†ì´ ì§„í–‰=====")
    

llm_with_params = base_llm
print("=====LLM ì„¤ì • ì™„ë£Œ=====")


import re
from typing import TypedDict, List
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END

parser = StrOutputParser()

# State ì •ì˜ 
class MCQState(TypedDict):
    id: str
    paragraph: str
    question: str
    choices: List[str]
    is_korean_history: bool # í•œêµ­ì‚¬ ë¬¸ì œì¸ì§€
    retrieved_context: str  # ìµœì¢… ìˆ˜ì§‘ëœ ì¦ê±° ìë£Œ
    full_response: str      # LLMì˜ í’€ì´ ê³¼ì • (Raw)
    final_answer: str       # ì¶”ì¶œëœ ì •ë‹µ ë²ˆí˜¸


ROUTER_PROMPT = PromptTemplate.from_template(
    """<|im_start|>system
ë‹¹ì‹ ì€ ê³¼ëª© ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë¬¸ì œê°€ "í•œêµ­ì‚¬" ê³¼ëª©ì˜ ë¬¸ì œì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

[ë¶„ë¥˜ ê¸°ì¤€]
- í•œêµ­ì‚¬ ì§€ì‹ì´ ê¼­ í•„ìš”í•œ ë¬¸ì œ: 'KOREAN_HISTORY'
- ì„¸ê³„ì‚¬, ì¼ë°˜ ë…¼ë¦¬, ë¬¸í•™, ë‹¨ìˆœ ìƒì‹ ë“±: 'GENERAL'

ê²°ê³¼ëŠ” ë°˜ë“œì‹œ 'KOREAN_HISTORY' ë˜ëŠ” 'GENERAL' ì¤‘ í•œ ë‹¨ì–´ë¡œë§Œ ë‹µí•˜ì„¸ìš”.
<|im_end|>
<|im_start|>user
[ì§€ë¬¸]
{paragraph}

[ì§ˆë¬¸]
{question}
<|im_end|>
<|im_start|>assistant
"""
)

def router_node(state: MCQState):
    """
    ==== í•œêµ­ì‚¬ ë¬¸ì œì¸ì§€ / ì•„ë‹Œì§€ ë¶„ê¸°í•˜ëŠ” ë…¸ë“œ =====
    """
    chain = ROUTER_PROMPT | llm_with_params | parser
    result = chain.invoke({
        "paragraph": state["paragraph"], 
        "question": state["question"]
    }).strip().upper()
    
    is_history = "KOREAN_HISTORY" in result
    
    return {"is_korean_history": is_history, "retrieved_context": ""}

# ë¶„ê¸° ê²°ì •ì„ ìœ„í•œ ì¡°ê±´ë¶€ í•¨ìˆ˜
def route_decision(state: MCQState):
    if state.get("is_korean_history", False):
        return "retrieve"
    return "general_solve"


def retrieve_node(state: MCQState):
    """
    ===== ë¬¸ì œ ìœ í˜•ë³„ ì¿¼ë¦¬ ìƒì„± ë° RAG ì‹¤í–‰ ë…¸ë“œ =====
    """

    # ë¬¸ì œ ìœ í˜• ë¶„ë¥˜
    router_prompt = PromptTemplate.from_template(
        """<|im_start|>system
ë‹¹ì‹ ì€ 'ë¬¸ì œ í•´ê²° ì „ëµê°€'ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì œë¥¼ ë³´ê³  ìµœì ì˜ ê²€ìƒ‰ ì „ëµì„ í•˜ë‚˜ ì„ íƒí•˜ì‹­ì‹œì˜¤.

[ë¶„ë¥˜ ê¸°ì¤€]
1. **INFERENCE**: (ê°€), (ë‚˜), "ì´ ì¸ë¬¼", "ì´ ë‹¨ì²´" ì²˜ëŸ¼ ì£¼ì–´ê°€ ê°€ë ¤ì ¸ ìˆì–´ ë¬¸ë§¥ ë¬˜ì‚¬ë¥¼ í†µí•´ ëŒ€ìƒì„ ì°¾ì•„ì•¼ í•˜ëŠ” ê²½ìš°.
2. **SEQUENCE**: "(ê°€)ì™€ (ë‚˜) ì‚¬ì´", "ìˆœì„œëŒ€ë¡œ ë‚˜ì—´", "ì—°í‘œ", "ì‹œê¸°" ë“± ì‹œê°„ì˜ íë¦„ì´ë‚˜ ì—°ë„ë¥¼ ë¬»ëŠ” ê²½ìš°.
3. **GENERAL**: ìœ„ ë‘ ê²½ìš°ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ì ì¸ ì‚¬ì‹¤ í™•ì¸ ë¬¸ì œ (ëŒ€ìƒì´ ëª…í™•í•œ ê²½ìš°).

ë°˜ë“œì‹œ **INFERENCE**, **SEQUENCE**, **GENERAL** ì¤‘ ë‹¨ì–´ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
<|im_start|>user
[ì§€ë¬¸]
{paragraph}

[ì§ˆë¬¸]
{question}

[ì„ ì§€]
{choices}

ì „ëµ:<|im_end|>
<|im_start|>assistant
"""
    )

    router_chain = router_prompt | llm_with_params | parser
    
    # ì „ëµ
    strategy = router_chain.invoke({
        "paragraph": state['paragraph'],
        "question": state['question'],
        "choices": str(state['choices'])
    }).strip()

    # ì „ëµì— ë”°ë¼ system messageë¥¼ ë‹¤ë¥´ê²Œ
    if "INFERENCE" in strategy:
        # INFERENCE: ëª½íƒ€ì£¼ ê²€ìƒ‰ (ì´ë¦„ ì¶”ì¸¡ ê¸ˆì§€)
        sys_msg = """ë‹¹ì‹ ì€ 'ì—­ì‚¬ íƒì •'ì´ë‹¤. [ì§€ë¬¸]ê³¼ [ì„ ì§€]ì„ ë³´ê³ , ìˆ¨ê²¨ì§„ ì£¼ì–´((ê°€), (ë‚˜) ë“±)ë¥¼ ì°¾ê¸° ìœ„í•œ 'ëª½íƒ€ì£¼ ê²€ìƒ‰ì–´'ë¥¼ ë§Œë“¤ì–´ë¼.
**ê·œì¹™** 
- (ê°€)ê°€ ëˆ„êµ¬ì¸ì§€ ì ˆëŒ€ ì¶”ì¸¡í•˜ì—¬ íŠ¹ì • ì´ë¦„(ì˜ˆ: ê¹€êµ¬, ì‹ ë¼)ì„ ë„£ì§€ë§ˆë¼.
- ì˜¤ì§ ì§€ë¬¸ì— ë¬˜ì‚¬ëœ 'í–‰ë™', 'ì‚¬ê±´ ë‚´ìš©', 'ë‹¨ì²´ëª…', 'ì¥ì†Œ'ë¥¼ ë‚˜ì—´í•˜ë¼.
- ê²€ìƒ‰ì–´ë§Œì„ ëŒ€ë‹µí•˜ì—¬ë¼.
"""

    elif "SEQUENCE" in strategy:
        # SEQUENCE: ì—°ë„/íƒ€ì„ë¼ì¸ ê²€ìƒ‰
        sys_msg = """ë‹¹ì‹ ì€ 'ì—°í‘œ ë¶„ì„ê°€'ì´ë‹¤. [ì§€ë¬¸]ê³¼ [ì„ ì§€]ì„ ë³´ê³ , ì‚¬ê±´ì˜ ìˆœì„œë‚˜ ì‹œê¸°ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•œ ê²€ìƒ‰ì–´ë¥¼ ë§Œë“¤ì–´ë¼.
**ê·œì¹™**
- ì§€ë¬¸ì— ë‚˜ì˜¨ ì‚¬ê±´ë“¤ì˜ 'ë°œìƒ ì—°ë„', 'ì‹œëŒ€ì  ë°°ê²½', 'ì™•ì˜ ì¬ìœ„ ê¸°ê°„' ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ë¼.
- ê²€ìƒ‰ì–´ë§Œì„ ëŒ€ë‹µí•˜ì—¬ë¼.
"""
    else: 
        # GENERAL: í•µì‹¬ ìš”ì•½ ê²€ìƒ‰
        sys_msg = """ë‹¹ì‹ ì€ 'ê²€ìƒ‰ í‚¤ì›Œë“œ ìš”ì•½ ì „ë¬¸ê°€'ì´ë‹¤. [ì§€ë¬¸]ê³¼ [ì„ ì§€]ì„ ë³´ê³ , ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ë¼.
**ê·œì¹™** 
- ë¶ˆí•„ìš”í•œ ì¡°ì‚¬ë¥¼ ë¹¼ê³ , 'ì¸ë¬¼', 'ì‚¬ê±´', 'í•µì‹¬ ìš©ì–´' ìœ„ì£¼ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ë¼.
- í‚¤ì›Œë“œë§Œì„ ëŒ€ë‹µí•˜ì—¬ë¼."""

    query_gen_prompt = PromptTemplate.from_template(
        """<|im_start|>system
{sys_msg}
<|im_start|>user
[ì§€ë¬¸]
{paragraph}

[ì„ ì§€]
{choices}

ìµœì  ê²€ìƒ‰ì–´:<|im_end|>
<|im_start|>assistant
"""
    )

    gen_chain = query_gen_prompt | llm_with_params | parser
    generated_query = gen_chain.invoke({
        "sys_msg": sys_msg,
        "paragraph": state['paragraph'],
        "choices": str(state['choices'])
    }).strip()

    print(f"[ìƒì„±ëœ ì¿¼ë¦¬]: {generated_query}")

    # [Dual Search] - llmì´ ë§Œë“  ì¿¼ë¦¬ë¡œ ì¦ê°•í•œ ê²°ê³¼ + ì§€ë¬¸ì„ ì¿¼ë¦¬ë¡œ ì¦ê°•í•œ ê²°ê³¼
    # llmì´ ë§Œë“  ì¿¼ë¦¬ë¡œ ì¦ê°•í•œ ê²°ê³¼
    llm_query_result = retriever.invoke(generated_query)

    # ì§€ë¬¸ì„ ì¿¼ë¦¬ë¡œ ì¦ê°•í•œ ê²°ê³¼
    para_query_result = retriever.invoke(state['paragraph'] + str(state['choices']))

    # ë‘ ê²€ìƒ‰ ê²°ê³¼ í•©ì¹˜ê¸° (ì •ë°€ ê²€ìƒ‰ ìš°ì„ )
    combined_docs = llm_query_result + para_query_result
    unique_docs = [] 
    seen = set()
    for d in combined_docs:  # ë‘ ê²°ê³¼ê°€ ê°€ì ¸ì˜¨ ë¬¸ì„œë“¤ ì¤‘ unique ë¬¸ì„œë“¤ë§Œ 
        if d.page_content not in seen:
            unique_docs.append(d)
            seen.add(d.page_content)

    # ----ë¦¬ë­ì»¤ ì‚¬ìš©í•˜ì—¬ ë³´ì™„í•  ì§€ì -----
    final_para_docs = unique_docs[:5] # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©
    para_context = "\n".join([f"- {d.page_content}" for d in final_para_docs])

    # # -----------------------------------------------------
    # # Phase 4: [Choices Search] ì„ ì§€ë³„ êµì°¨ ê²€ì¦
    # # -----------------------------------------------------
    # choices_evidence = []
    # for idx, choice in enumerate(state['choices']):
    #     # ê²€ìƒ‰ì–´ = ìµœì í™”ëœ ì¿¼ë¦¬ + ì„ ì§€ ë‚´ìš©
    #     combined_q = f"{generated_query} {choice}"
    #     choice_docs = retriever.invoke(combined_q)

    #     if choice_docs:
    #         evi = " / ".join([d.page_content for d in choice_docs[:2]]) # ì„ ì§€ë‹¹ 2ê°œë§Œ
    #     else:
    #         evi = "ê´€ë ¨ ì •ë³´ ì—†ìŒ"
    #     choices_evidence.append(f"[ì„ ì§€ {idx+1}]: {evi}")

    # -----------------------------------------------------
    # Phase 5: ë¬¸ë§¥ ì¡°ë¦½
    # -----------------------------------------------------
    full_context = f"""
=== [ë°°ê²½ ì§€ì‹ (ì „ëµ: {strategy})] ===
{para_context}
"""
    return {"retrieved_context": full_context}


# =========================================================
# 3. Solver Node (ë¬¸ì œ í’€ì´)
# =========================================================
def ko_history_solver_node(state: MCQState):
    """
    í•œêµ­ì‚¬ ë¬¸ì œ í’€ì´ ë…¸ë“œ 
    """
    choices_str = "\n".join(
        [f"{i+1}. {c}" for i, c in enumerate(state['choices'])]
    )

    system_prompt = """ë‹¹ì‹ ì€ ë…¼ë¦¬ì ì¸ í•œêµ­ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì œê³µëœ <ê°œë… ë³´ì¶© ìë£Œ>ë¥¼ ê·¼ê±°ë¡œ ë¬¸ì œë¥¼ í•´ê²°í•˜ì‹­ì‹œì˜¤.
- ìë£Œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ê³ , ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ ì˜¤ë‹µì„ ì†Œê±°í•˜ì‹­ì‹œì˜¤.
- ë§ˆì§€ë§‰ ì¤„ì—ëŠ” ë°˜ë“œì‹œ {"ì •ë‹µ": "ë²ˆí˜¸"} í˜•ì‹ìœ¼ë¡œ ë‹µì„ ì¶œë ¥í•˜ì‹­ì‹œì˜¤."""

    prompt = PromptTemplate.from_template(
        """<|im_start|>system
{system_msg}<|im_end|>
<|im_start|>user
<ì§€ë¬¸>
{paragraph}

<ê°œë… ë³´ì¶© ìë£Œ>
{retrieved_context}

<ì§ˆë¬¸>
{question}

<ì„ ì§€>
{choices}
<|im_end|>
<|im_start|>assistant
"""
    )

    chain = prompt | llm_with_params | parser

    response = chain.invoke({
        "system_msg": system_prompt,
        "paragraph": state['paragraph'],
        "question": state['question'],
        "choices": choices_str,
        "retrieved_context": state['retrieved_context']
    })

    return {"full_response": response}


def general_solver_node(state: MCQState):
    """
    í•œêµ­ì‚¬ë¥¼ ì œì™¸í•œ ê³¼ëª© ë¬¸ì œ í’€ì´ ë…¸ë“œ
    """
    choices_str = "\n".join([f"{i+1}. {c}" for i, c in enumerate(state['choices'])])
    prompt = PromptTemplate.from_template(
      """<|im_start|>system
      ë‹¹ì‹ ì€ ë…¼ë¦¬ì ì´ê³  ê¼¼ê¼¼í•œ í•™ìƒì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì œì‹œë¬¸ê³¼ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°ê´€ì‹ ë¬¸ì œë¥¼ í•´ê²°í•´ì•¼ í•©ë‹ˆë‹¤.

### ì§€ì‹œì‚¬í•­
1. **ë¬¸ì œ ë¶„ì„**: ì§ˆë¬¸ì´ ìš”êµ¬í•˜ëŠ” í•µì‹¬ì´ ë¬´ì—‡ì¸ì§€ ë¨¼ì € ì •ì˜í•˜ì‹­ì‹œì˜¤.
2. **ì‚¬ê³  ê³¼ì • (CoT)**:
   - ê° ì„ íƒì§€ê°€ ì •ë‹µì´ê±°ë‚˜ ì˜¤ë‹µì¸ ì´ìœ ë¥¼ ì œì‹œë¬¸ì—ì„œ ê·¼ê±°ë¥¼ ì°¾ì•„ ëª…í™•íˆ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.
   - ë‹¨ìˆœíˆ ì •ë‹µë§Œ ë§íˆì§€ ë§ê³ , ì™œ ë‚˜ë¨¸ì§€ ì„ íƒì§€ëŠ” ì •ë‹µì´ ë  ìˆ˜ ì—†ëŠ”ì§€(ì˜¤ë‹µ ì†Œê±°)ë¥¼ ë…¼ë¦¬ì ìœ¼ë¡œ ì„œìˆ í•˜ì‹­ì‹œì˜¤.
3. **í˜•ì‹ ì¤€ìˆ˜**:
   - í’€ì´ ê³¼ì •ì€ ì¤„ê¸€ë¡œ ì‘ì„±í•˜ë˜, ë¶ˆí•„ìš”í•œ ë°˜ë³µì„ í”¼í•˜ì‹­ì‹œì˜¤.
   - **ê°€ì¥ ë§ˆì§€ë§‰ ì¤„**ì—ëŠ” ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ ë‹µì•ˆì„ ì œì¶œí•˜ì‹­ì‹œì˜¤.
{{"ì •ë‹µ": "ë²ˆí˜¸"}}
<|im_end|>
<|im_start|>user
[ì§€ë¬¸]
{paragraph}

[ì§ˆë¬¸]
{question}

[ì„ ì§€]
{choices}
<|im_end|>
<|im_start|>assistant
"""
    )
    response = (prompt | llm_with_params | parser).invoke({
        "paragraph": state['paragraph'],
        "question": state['question'],
        "choices": choices_str
    })

    return {'full_response': response}


def parser_node(state: MCQState):
    """
    ì •ë‹µ ì¶”ì¶œ ë…¸ë“œ
    """
    text = state['full_response']
    answer = None
    # JSON í˜•ì‹ ì°¾ê¸°
    match = re.search(r'{"ì •ë‹µ":\s*"(\d+)"}', text)
    if match:
        answer = match.group(1)
    else:
        # ëª» ì°¾ìœ¼ë©´ í…ìŠ¤íŠ¸ ë‚´ ë§ˆì§€ë§‰ ìˆ«ì ì¶”ì¶œ (ìµœí›„ì˜ ìˆ˜ë‹¨)
        nums = re.findall(r'\d+', text)
        if nums: answer = nums[-1]
        
    return {"final_answer": answer}


# ì›Œí¬í”Œë¡œìš° ì¡°ë¦½
workflow = StateGraph(MCQState)

workflow.add_node("router", router_node)
workflow.add_node("retrieve", retrieve_node)
workflow.add_node("korean_solve", ko_history_solver_node)
workflow.add_node("general_solve", general_solver_node)
workflow.add_node("parse", parser_node)

# íë¦„ ì—°ê²°
workflow.set_entry_point("router")

workflow.add_conditional_edges(
    "router",
    route_decision,
    {
        "retrieve": "retrieve",
        "general_solve": "general_solve"
    }
)

# í•œêµ­ì‚¬ ê²½ë¡œ: Retrieve -> Korean Solve -> Parse
workflow.add_edge("retrieve", "korean_solve")
workflow.add_edge("korean_solve", "parse")

# ì¼ë°˜ ê²½ë¡œ: General Solve -> Parse
workflow.add_edge("general_solve", "parse")

# ì¢…ë£Œ
workflow.add_edge("parse", END)

# ì»´íŒŒì¼
app = workflow.compile()
print("=====ì›Œí¬í”Œë¡œìš° ì¡°ë¦½ ì™„ë£Œ=====")

def main():
    # ë°ì´í„° ë¡œë“œ (73ê°œ í…ŒìŠ¤íŠ¸)
    csv_path = "...csv ê²½ë¡œ..."
    is_train = os.path.basename(csv_path).startswith("train")
    
    data = pd.read_csv(csv_path)

    results = []
    sub = []  # ìµœì¢… ì œì¶œ í˜•ì‹ csv ìš©
    print("ğŸ”¥ ìµœì¢… RAG ëª¨ë¸ í‰ê°€ ì‹œì‘...")

    for idx, row in tqdm(data.iterrows(), total=len(data)):
        try:
            # ë°ì´í„° ì „ì²˜ë¦¬
            problem = ast.literal_eval(row['problems']) if isinstance(row['problems'], str) else row['problems']

            inputs = {
                "id": row['id'],
                "paragraph": row['paragraph'],
                "question": problem['question'],
                "choices": problem['choices']
            }

            # LangFuse ì½œë°± ì„¤ì •
            config = {"callbacks": [langfuse_handler]} if 'langfuse_handler' in globals() else {}

            # ê·¸ë˜í”„ ì‹¤í–‰
            output = app.invoke(inputs, config=config)

            # ê²°ê³¼ ì €ì¥
            result = {
                "id": inputs['id'],
                "question": inputs['question'],
                "predicted_answer": output['final_answer'],
                "retrieved_context": output['retrieved_context'],
                "full_response": output['full_response'],
            }

            # train.csvì¼ ë•Œë§Œ ì‹¤ì œ answer ê´€ë ¨ í•„ë“œ ì¶”ê°€
            if is_train:
                ground_truth = problem['answer']
                result.update({
                    "real_answer": str(ground_truth),
                    "is_correct": str(output['final_answer']) == str(ground_truth)
                })

            results.append(result)

            sub.append({
                "id": inputs['id'],
                "answer": output['final_answer']
            })

        except Exception as e:
            print(f"âŒ Error at {idx}: {e}")

    # ê²°ê³¼ ì €ì¥ ë° ì¶œë ¥
    final_df = pd.DataFrame(results)
    final_df.to_csv("/content/drive/MyDrive/csat/rag_results.csv", index=False, encoding='utf-8-sig')

    sub_df = pd.DataFrame(sub)
    sub_df.to_csv("/content/drive/MyDrive/csat/sub_lang.csv", index=False, encoding='utf-8-sig')

    acc = final_df['is_correct'].mean() * 100
    print(f"\nğŸ† ìµœì¢… ì •ë‹µë¥ : {acc:.2f}%")
    
if __name__ == "__main__":
    main()



