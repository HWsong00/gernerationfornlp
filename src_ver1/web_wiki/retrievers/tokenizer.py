import re
from typing import List, Optional
from kiwipiepy import Kiwi

# ê¸°ë³¸ íƒœê·¸ ì„¤ì • (í•„ìš”ì‹œ settings.pyì—ì„œ ê´€ë¦¬í•´ë„ ì¢‹ìŠµë‹ˆë‹¤)
DEFAULT_TAG_INCLUDE = [
    'NNG', 'NNP', 'NNB', 'NR', 'VV', 'VA', 'MM', 'XR', 
    'SW', 'SL', 'SH', 'SN', 'SB'
]

# 1. Kiwi ì‹±ê¸€í†¤ íŒ©í† ë¦¬
_kiwi_instance = None

def get_kiwi():
    """Kiwi ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë‹¨ í•˜ë‚˜ë§Œ ìƒì„±í•˜ì—¬ ê³µìœ í•©ë‹ˆë‹¤."""
    global _kiwi_instance
    if _kiwi_instance is None:
        print("ğŸ” [Tokenizer] Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ë¡œë“œ ì¤‘...")
        _kiwi_instance = Kiwi()
    return _kiwi_instance

def _fallback_tokenize(text: str) -> List[str]:
    """ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì•ˆì „ì¥ì¹˜"""
    return re.findall(r'\b\w+\b', text, re.UNICODE)

# 2. í•µì‹¬ í† í°í™” ë¡œì§ (ì¥(Jang)ë‹˜ì˜ ë¡œì§ ìœ ì§€)
def tokenize_kiwi(
    text: str,
    text_type: str, # "corpus" ë˜ëŠ” "query"
    kiwi: Optional[Kiwi] = None,
    tag_include: Optional[List[str]] = None,
    top_n: int = 3,
    score_threshold: float = 1.2,
) -> List[str]:
    # ì¸ìê°€ ì—†ìœ¼ë©´ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ì™€ ê¸°ë³¸ íƒœê·¸ ì‚¬ìš©
    kiwi = kiwi or get_kiwi()
    tag_include = tag_include or DEFAULT_TAG_INCLUDE
    
    try:
        if text_type == "corpus":
            # ìƒ‰ì¸ ì‹œ: ë³¸ë¬¸ ê¸¸ì´ì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ í›„ë³´êµ° í™•ì¥
            analyzed = kiwi.analyze(text, top_n=top_n + len(text) // 200)
            if not analyzed: return _fallback_tokenize(text)
            
            num_candi = 1
            # 1ìœ„ ëŒ€ë¹„ ì ìˆ˜ì°¨ê°€ í¬ì§€ ì•Šì€ í›„ë³´ë“¤ í¬í•¨ (ì¬í˜„ìœ¨ í™•ë³´)
            while (num_candi < len(analyzed) and 
                   analyzed[num_candi][1] > score_threshold * analyzed[0][1]):
                num_candi += 1
                
        elif text_type == "query":
            # ê²€ìƒ‰ ì‹œ: ì •ë°€í•œ ìƒìœ„ í›„ë³´ ì‚¬ìš©
            analyzed = kiwi.analyze(text, top_n=top_n)
            if not analyzed: return _fallback_tokenize(text)
            num_candi = min(3, len(analyzed))

        # í˜•íƒœì†Œ/íƒœê·¸ ê²°í•© ì¶”ì¶œ
        all_tokenized = [
            f"{t.form}/{t.tag}"
            for nc in range(num_candi)
            for t in analyzed[nc][0]
            if t.tag in tag_include
        ]

        unique_tokens = list(set(all_tokenized))
        return unique_tokens if unique_tokens else _fallback_tokenize(text)
    
    except Exception as e:
        print(f"âš ï¸ [Tokenizer] ì—ëŸ¬ ë°œìƒ: {e}")
        return _fallback_tokenize(text)