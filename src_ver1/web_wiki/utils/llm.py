import os
import time
import subprocess
import requests
from openai import OpenAI

# 1. í”„ë¡œì íŠ¸ ì „ì—­ì—ì„œ ì‚¬ìš©í•  ëª¨ë¸ ì‹ë³„ì (llama-serverëŠ” ì–´ë–¤ ë¬¸ìì—´ì´ë“  ìˆ˜ìš©í•©ë‹ˆë‹¤)
MODEL_NAME = "Qwen3-30B-A3B-Instruct-2507"

from openai import OpenAI

# ì„œë²„ ì‹¤í–‰ ì‹œ ì„¤ì •í•œ í¬íŠ¸ì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤!
PORT = 8081 

def get_llm_client():
    """llama-serverì™€ í†µì‹ í•˜ëŠ” Native OpenAI í´ë¼ì´ì–¸íŠ¸"""
    client = OpenAI(
        base_url=f"http://localhost:{PORT}/v1", 
        api_key="sk-no-key-required"
    )
    return client

def check_server_status():
    import requests
    try:
        # í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸ë„ ê°™ì€ í¬íŠ¸ë¥¼ ë°”ë¼ë´ì•¼ í•¨
        response = requests.get(f"http://localhost:{PORT}/health")
        return response.status_code == 200
    except:
        return False

def start_llama_server():
    """ì„œë²„ê°€ ì—†ìœ¼ë©´ ì§ì ‘ nohupìœ¼ë¡œ ì‹¤í–‰"""
    if check_server_status():
        print("âœ… ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
        return None

    print("ğŸš€ ì„œë²„ê°€ êº¼ì ¸ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ê°€ë™í•©ë‹ˆë‹¤...")
    # ì§ˆë¬¸ìë‹˜ì´ ì•„ê¹Œ ì“°ì…¨ë˜ ê·¸ ëª…ë ¹ì–´ì…ë‹ˆë‹¤.
    cmd = f"""nohup /content/llama-server \
        --model "models/Qwen3-30B-A3B-Instruct-2507-UD-Q6_K_XL.gguf" \
        --n-gpu-layers -1 \
        --ctx-size 14400 \
        --parallel 2 \
        --cont-batching \
        --flash-attn on \
        --port {PORT} \
        --host 0.0.0.0 \
        > server.log 2>&1 &"""
    
    # ì…¸ ëª…ë ¹ì–´ë¡œ ì‹¤í–‰
    process = subprocess.Popen(cmd, shell=True)
    
    # ì„œë²„ê°€ ì¤€ë¹„ë  ë•Œê¹Œì§€ ëŒ€ê¸° 
    for i in range(240):
        if check_server_status():
            print("âœ… ì„œë²„ ë¡œë“œ ì™„ë£Œ!")
            return process
        if i % 5 == 0:
            print(f"â³ ëª¨ë¸ ë¡œë”© ì¤‘... ({i*5}ì´ˆ ê²½ê³¼)")
        time.sleep(5)
    
    raise TimeoutError("ì„œë²„ ê°€ë™ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. server.logë¥¼ í™•ì¸í•˜ì„¸ìš”.")


if __name__ == "__main__":
    # íŒŒì¼ ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸
    if check_server_status():
        print("âœ… llama-serverê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.")
        client = get_llm_client()
        print(f"ğŸš€ í´ë¼ì´ì–¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ (Base URL: {client.base_url})")
    else:
        print("âŒ ì„œë²„ ì—°ê²° ì‹¤íŒ¨. llama-serverê°€ 8080 í¬íŠ¸ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.")