import gc
import torch
import json
from nodes.state import MCQState
from utils.llm import get_llm_client, MODEL_NAME
from utils.wiki import WikipediaAPI, WikiChunker
from utils.reranker import Reranker

def retrieve_node(state: MCQState, ensemble_retriever, reranker):
    client = get_llm_client()
    
    # [ìˆ˜ì •] stateì—ì„œ êº¼ë‚´ì§€ ë§ê³ , ì¸ìë¡œ ë°›ì€ ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ë³€ìˆ˜ëª…ì„ ê¸°ì¡´ ì½”ë“œì™€ ë§ì¶°ì£¼ë©´ ì•„ë˜ ë¡œì§ì„ ê³ ì¹  í•„ìš”ê°€ ì—†ì–´ í¸ë¦¬í•©ë‹ˆë‹¤.
    reranker_obj = reranker
    retriever_obj = ensemble_retriever

    # --- Phase 1: ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ (Robust Version) ---
    system_content = (
        "ë‹¹ì‹ ì€ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ {\"keywords\": [\"ìš©ì–´1\", \"ìš©ì–´2\"]} í˜•ì‹ì˜ JSONìœ¼ë¡œë§Œ ë‹µë³€í•˜ì‹­ì‹œì˜¤."
    )
    messages = [{"role": "system", "content": system_content},
                {"role": "user", "content": f"ì§€ë¬¸: {state.get('paragraph', '')}\nì§ˆë¬¸: {state.get('question', '')}"}]

    try:
        kw_response = client.chat.completions.create(
            model=MODEL_NAME, messages=messages, temperature=0,
            response_format={"type": "json_object"}
        )
        parsed_json = json.loads(kw_response.choices[0].message.content)
        
        # ì–´ë–¤ êµ¬ì¡°ë¡œ ì˜¤ë“  ë¬´ì¡°ê±´ List[str] ë³´ì¥
        raw_val = parsed_json.get('keywords', list(parsed_json.values())[0] if parsed_json else [])
        search_queries = raw_val if isinstance(raw_val, list) else [str(raw_val)]
        search_queries = [q.strip() for q in search_queries if len(q.strip()) > 1][:3]
    except Exception as e:
        print(f"âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        search_queries = [state.get('question', '')[:20]]

    print(f"ğŸ” [Retriever] ìµœì¢… ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸: {search_queries}")

    # --- Phase 2: ë‹¤ì¤‘ ì¶œì²˜ ê²€ìƒ‰ (Efficiency Optimized) ---
    candidate_docs = []
    # [ìˆ˜ì •] API ê°ì²´ëŠ” ë£¨í”„ ë°–ì—ì„œ í•œ ë²ˆë§Œ ìƒì„±
    wiki_api = WikipediaAPI()
    chunker = WikiChunker()

    for q in search_queries:
        candidate_docs.extend(retriever_obj.invoke_ensemble(q))
        try:
            wiki_raw = wiki_api.search_and_fetch([q])
            if wiki_raw:
                # [ìˆ˜ì •] ìœ„í‚¤ ë¬¸ì„œê°€ ë„ˆë¬´ ë§ì•„ì§€ì§€ ì•Šê²Œ ì¡°ì ˆ
                wiki_chunks = chunker.chunk(wiki_raw)
                for ch in wiki_chunks[:5]: # ì¿¼ë¦¬ë‹¹ ìœ„í‚¤ ì²­í¬ëŠ” 5ê°œë¡œ ì œí•œ
                    candidate_docs.append(ch['text'])
        except: pass

    # ì¤‘ë³µ ì œê±° ë° [í•µì‹¬] ê¸¸ì´ ì œí•œ(Truncation)
    raw_texts = []
    seen = set()
    for d in candidate_docs:
        text = d.page_content if hasattr(d, 'page_content') else str(d)
        if text not in seen:
            # [ìˆ˜ì •] ë¦¬ë­ì»¤ VRAM ë³´í˜¸ë¥¼ ìœ„í•´ ë¬¸ì„œë‹¹ 1000ìë¡œ ì œí•œ
            raw_texts.append(text[:1000]) 
            seen.add(text)

    # --- Phase 3: ë¦¬ë­í‚¹ ë° ë©”ëª¨ë¦¬ ì •ë¦¬ ---
    combined_query = f"{state['question']} {' '.join(state['choices'])}"
    print(f"âš–ï¸ [Reranker] {len(raw_texts)}ê°œ ë¬¸ì„œ ì¬ì •ë ¬ ì‹œì‘")
    
    reranked_results = reranker_obj.rerank(combined_query, raw_texts, top_k=3)
    final_context = [f"[{i+1}] (ì‹ ë¢°ë„: {score:.2f}) {text}" for i, (text, score) in enumerate(reranked_results)]

    # ë©”ëª¨ë¦¬ ì •ë¦¬
    del candidate_docs, raw_texts
    gc.collect()
    torch.cuda.empty_cache()

    # ì´ ë¶€ë¶„ì´ ìµœì¢…ì…ë‹ˆë‹¤. ë” ì´ìƒ ìˆ˜ì • ì•ˆ í•˜ì…”ë„ ë©ë‹ˆë‹¤!
    return {
        "retrieved_context": f"=== [ì—„ì„ ëœ ì§€ì‹ ì»¨í…ìŠ¤íŠ¸] ===\n" + "\n\n".join(final_context),
        "optimized_query": ", ".join(search_queries) # í‚¤ ì´ë¦„ì„ Stateì™€ ì¼ì¹˜ì‹œí‚´
    }